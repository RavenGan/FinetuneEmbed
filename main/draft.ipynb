{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9A4710B9-0DA3-36BB-9129-645F282E64B2> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <ECC148AF-20FF-3EEE-BC75-4DD3E7455393> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom classification model with SBERT embeddings\n",
    "class SBERTClassifier(nn.Module):\n",
    "    def __init__(self, sbert_model):\n",
    "        super(SBERTClassifier, self).__init__()\n",
    "        self.sbert_model = sbert_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.sbert_model.get_sentence_embedding_dimension(), 1),\n",
    "            nn.Sigmoid()  # Sigmoid to output probabilities between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_texts):\n",
    "        # Encode the input texts using SBERT\n",
    "        embeddings = self.sbert_model.encode(input_texts, convert_to_tensor=True)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Dataset class\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample data (replace with your actual dataset)\n",
    "train_texts = [\"I love programming\", \"Python is great\", \"I dislike AI\", \"Coding is fun\"]\n",
    "train_labels = [1, 1, 0, 1]  # Positive class\n",
    "val_texts = [\"I hate bugs\", \"Debugging is frustrating\", \"I like eating\"]\n",
    "val_labels = [0, 0, 1]  # Negative class\n",
    "test_texts = [\"Learning AI is amazing\", \"Errors are annoying\"]\n",
    "test_labels = [1, 0]\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "test_dataset = TextDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = SBERTClassifier(sbert_model).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_texts).squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for texts, labels in train_loader:\n",
    "    print(texts)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Training function with AUC calculation on validation set\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for texts, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            texts = [text for text in texts]\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts).squeeze()  # Remove extra dimension\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validate after each epoch\n",
    "        auc = evaluate_model(model, val_loader)\n",
    "        print(f\"Validation AUC after Epoch {epoch + 1}: {auc:.4f}\")\n",
    "\n",
    "# Evaluation function for AUC\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = [text for text in texts]\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get predicted probabilities\n",
    "            outputs = model(texts).squeeze()  # Remove extra dimension\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "\n",
    "            outputs = outputs.cpu().detach().numpy()\n",
    "            if outputs.ndim == 0:\n",
    "                outputs = outputs.reshape(1)\n",
    "\n",
    "            print(outputs)\n",
    "            all_probs.extend(outputs)\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    model.train()  # Set model back to training mode\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function to calculate AUC on the test set\n",
    "def test_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = [text for text in texts]\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(texts).squeeze()\n",
    "            all_labels.extend(labels.cpu().detach().numpy())\n",
    "            all_probs.extend(outputs.cpu().detach().numpy())\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    print(f\"Test AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 2/2 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7047\n",
      "[0.4892356 0.4817939]\n",
      "[0.49916002]\n",
      "Validation AUC after Epoch 1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 49.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.7047\n",
      "[0.48924395 0.48181123]\n",
      "[0.49917406]\n",
      "Validation AUC after Epoch 2: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 27.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.7046\n",
      "[0.48925275 0.48182866]\n",
      "[0.49918854]\n",
      "Validation AUC after Epoch 3: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 53.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.7045\n",
      "[0.48926187 0.4818462 ]\n",
      "[0.4992033]\n",
      "Validation AUC after Epoch 4: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 53.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.7045\n",
      "[0.48927328 0.48186508]\n",
      "[0.49922004]\n",
      "Validation AUC after Epoch 5: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 28.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48928145 0.48188192]\n",
      "[0.49923393]\n",
      "Validation AUC after Epoch 6: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4892903 0.4818994]\n",
      "[0.4992484]\n",
      "Validation AUC after Epoch 7: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.7042\n",
      "[0.4893012 0.4819178]\n",
      "[0.49926472]\n",
      "Validation AUC after Epoch 8: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 51.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.7042\n",
      "[0.4893101 0.4819353]\n",
      "[0.49927923]\n",
      "Validation AUC after Epoch 9: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.7041\n",
      "[0.489321   0.48195368]\n",
      "[0.49929544]\n",
      "Validation AUC after Epoch 10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
