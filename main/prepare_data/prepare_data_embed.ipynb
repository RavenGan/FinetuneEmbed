{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "# Specify the working directory\n",
    "os.chdir('/Users/david/Desktop/FinetuneEmbed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a1c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gene ncbi descriptions\n",
    "with open(\"./data/gene_text/hs_ncbi_gene_text.json\", \"r\") as file:\n",
    "    gene_descriptions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57a560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9A4710B9-0DA3-36BB-9129-645F282E64B2> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <ECC148AF-20FF-3EEE-BC75-4DD3E7455393> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Initialize the augmenter\n",
    "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_texts(texts, labels, genes, num_augmented=3):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    augmented_genes = []\n",
    "    \n",
    "    for text, label, gene in zip(texts, labels, genes):\n",
    "        # Add the original text to the augmented dataset\n",
    "        augmented_texts.append(text)\n",
    "        augmented_labels.append(label)\n",
    "        augmented_genes.append(gene)\n",
    "        \n",
    "        # Generate augmented versions of the text\n",
    "        for _ in range(num_augmented):\n",
    "            augmented_text = synonym_aug.augment(text)\n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(label)\n",
    "            augmented_genes.append(gene)\n",
    "    \n",
    "    return augmented_texts, augmented_labels, augmented_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e05ec",
   "metadata": {},
   "source": [
    "## Long- vs short- range TFs\n",
    "The input data used here are downloaded from Chen et al. (2020) (link: https://www-nature-com.stanford.idm.oclc.org/articles/s41467-020-16106-x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_short_range_tf = pd.read_csv('./data/long_vs_shortTF/41467_2020_16106_MOESM4_ESM.csv')\n",
    "# long_range_tf_gene = list(long_short_range_tf[long_short_range_tf['assignment']=='long-range TF']\\\n",
    "#                                 ['Unnamed: 0'])\n",
    "# long_range_tf_gene = list(set(long_range_tf_gene) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "\n",
    "# short_range_tf_gene = list(long_short_range_tf[long_short_range_tf['assignment']=='short-range TF']\\\n",
    "#                                 ['Unnamed: 0'])\n",
    "# short_range_tf_gene = list(set(short_range_tf_gene) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59d202",
   "metadata": {},
   "source": [
    "The input data used here are downloaded from the geneformer paper Hugging Face website (link: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/example_input_files/gene_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/long_vs_shortTF/example_input_files_gene_classification_tf_regulatory_range_tf_regulatory_range.pickle\", \"rb\") as f:\n",
    "    check_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af92252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biothings.client:querying 1-46...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:Finished.\n",
      "INFO:biothings.client:querying 1-130...\n",
      "INFO:biothings.client:done.\n",
      "INFO:biothings.client:Finished.\n",
      "WARNING:biothings.client:2 input query terms found no hit:\t['ENSG00000269603', 'ENSG00000267841']\n",
      "INFO:biothings.client:Pass \"returnall=True\" to return complete lists of duplicate or missing query terms.\n"
     ]
    }
   ],
   "source": [
    "long_range_tf_gene = check_data['long_range']\n",
    "short_range_tf_gene = check_data['short_range']\n",
    "\n",
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "long_range_query = mg.querymany(long_range_tf_gene, species='human')\n",
    "short_range_query = mg.querymany(short_range_tf_gene, species='human')\n",
    "long_range_gene_name = [x['symbol'] for x in long_range_query]\n",
    "short_range_gene_name = [x['symbol'] for x in short_range_query if 'symbol' in x]\n",
    "\n",
    "long_range_tf_gene = list(set(long_range_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "short_range_tf_gene = list(set(short_range_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394d091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import GeneDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf_genes = long_range_tf_gene + short_range_tf_gene\n",
    "labels = [1] * len(long_range_tf_gene) + [0] * len(short_range_tf_gene) # 1 for long-range TF, 0 for short-range TF\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(tf_genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084d5ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 139, 139)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_train), len(genes_train), len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85457686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the training texts\n",
    "desc_train_aug, labels_train_aug, genes_train_aug = augment_texts(desc_train, labels_train, genes_train, num_augmented=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac499cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556, 556, 556)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_train_aug), len(genes_train_aug), len(labels_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a394da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import collate_fn\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GeneDataset(genes_train_aug, desc_train_aug, labels_train_aug)\n",
    "val_dataset = GeneDataset(genes_test, desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train_aug, 'dataloader':train_loader, 'labels':labels_train_aug}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d2933a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m genes, labels, descs \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(genes)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/FinetuneEmbed/mod/mod.py:56\u001b[0m, in \u001b[0;36mGeneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     55\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescriptions[idx]\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m\"\u001b[39m: gene, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for genes, labels, descs in train_loader:\n",
    "    print(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b700f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/long_vs_shortTF/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/long_vs_shortTF/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe13b8b",
   "metadata": {},
   "source": [
    "## Dosage sensitive vs insensitive TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_file = \"https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/raw/main/example_input_files/gene_classification/dosage_sensitive_tfs/dosage_sens_tf_labels.csv\"\n",
    "with open(f\"./data/DosageSensitivity/example_input_files_gene_classification_dosage_sensitive_tfs_dosage_sensitivity_TFs.pickle\", \"rb\") as fp:\n",
    "    dosage_tfs = pickle.load(fp)\n",
    "sensitive = dosage_tfs[\"Dosage-sensitive TFs\"]\n",
    "insensitive = dosage_tfs[\"Dosage-insensitive TFs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075909b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 input query terms found no hit:\t['ENSG00000215271']\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "sensitive_query = mg.querymany(sensitive, species='human')\n",
    "in_sensitive_query = mg.querymany(insensitive, species='human')\n",
    "sensitive_gene_name = [x['symbol'] for x in sensitive_query]\n",
    "insensitive_gene_name = [x['symbol'] for x in in_sensitive_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39656ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_gene = list(set(sensitive_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "insensitive_gene = list(set(insensitive_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = sensitive_gene + insensitive_gene\n",
    "labels = [1] * len(sensitive_gene) + [0] * len(insensitive_gene) # 1 for sensitive, 0 for insensitive\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/DosageSensitivity/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/DosageSensitivity/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083924a",
   "metadata": {},
   "source": [
    "## Methylation state prediction\n",
    "The csv files are downloaded from https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa2f48",
   "metadata": {},
   "source": [
    "#### Bivalent vs. lys4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/MethylationState/example_input_files_gene_classification_bivalent_promoters_bivalent_vs_lys4_only.pickle\", \"rb\") as fp:\n",
    "    bivalent_vs_lys4 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene_labels = bivalent_vs_lys4['bivalent']\n",
    "lysine_gene_labels = bivalent_vs_lys4['lys4_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c068c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 input query terms found dup hits:\t[('ENSG00000007372', 2), ('ENSG00000110693', 2), ('ENSG00000117707', 2), ('ENSG00000120093', 2), ('E\n",
      "2 input query terms found dup hits:\t[('ENSG00000196628', 2), ('ENSG00000198728', 2)]\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "bivalent_query = mg.querymany(bivalent_gene_labels, species='human')\n",
    "lysine_query = mg.querymany(lysine_gene_labels, species='human')\n",
    "bivalent_gene_name = [x.get('symbol', '') for x in bivalent_query]\n",
    "lysine_gene_name = [x.get('symbol', '') for x in lysine_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene = list(set(bivalent_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "lysine_gene = list(set(lysine_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c153053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = bivalent_gene + lysine_gene\n",
    "labels = [1] * len(bivalent_gene) + [0] * len(lysine_gene) # 1 for bivalent_gene, 0 for lysine_gene\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733796f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/MethylationState/bivalent_vs_lys4/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/MethylationState/bivalent_vs_lys4/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22624ec0",
   "metadata": {},
   "source": [
    "#### Bivalent vs. no methyl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62be879",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/MethylationState/example_input_files_gene_classification_bivalent_promoters_bivalent_vs_no_methyl.pickle\", \"rb\") as fp:\n",
    "    bivalent_vs_no_methyl = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1863d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene_labels = bivalent_vs_no_methyl['bivalent']\n",
    "no_methylation_gene_labels = bivalent_vs_no_methyl['no_methylation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 input query terms found dup hits:\t[('ENSG00000007372', 2), ('ENSG00000110693', 2), ('ENSG00000117707', 2), ('ENSG00000120093', 2), ('E\n",
      "2 input query terms found dup hits:\t[('ENSG00000147488', 2), ('ENSG00000151322', 2)]\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "bivalent_query = mg.querymany(bivalent_gene_labels, species='human')\n",
    "no_methylation_query = mg.querymany(no_methylation_gene_labels, species='human')\n",
    "bivalent_gene_name = [x.get('symbol', '') for x in bivalent_query]\n",
    "no_methylation_gene_name = [x.get('symbol', '') for x in no_methylation_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene = list(set(bivalent_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "no_methylation_gene = list(set(no_methylation_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = bivalent_gene + no_methylation_gene\n",
    "labels = [1] * len(bivalent_gene) + [0] * len(no_methylation_gene) # 1 for bivalent_gene, 0 for no_methylation_gene\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/MethylationState/bivalent_vs_no_methyl/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/MethylationState/bivalent_vs_no_methyl/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce388717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06603c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'genes': ['gene2', 'gene1'], 'labels': tensor([1, 0]), 'descriptions': tensor([[ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 112, 108,  97, 121,\n",
      "         115,  32,  97,  32, 114, 111, 108, 101,  32, 105, 110,  32,  99, 101,\n",
      "         108, 108,  32, 100, 105, 118, 105, 115, 105, 111, 110,  46,   0,   0,\n",
      "           0,   0,   0],\n",
      "        [ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 105, 115,  32, 105,\n",
      "         110, 118, 111, 108, 118, 101, 100,  32, 105, 110,  32, 109, 101, 116,\n",
      "          97,  98, 111, 108, 105,  99,  32, 112, 114, 111,  99, 101, 115, 115,\n",
      "         101, 115,  46]])}\n",
      "{'genes': ['gene3'], 'labels': tensor([0]), 'descriptions': tensor([[ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 105, 115,  32,  97,\n",
      "         115, 115, 111,  99, 105,  97, 116, 101, 100,  32, 119, 105, 116, 104,\n",
      "          32, 116, 104, 101,  32, 105, 109, 109, 117, 110, 101,  32, 114, 101,\n",
      "         115, 112, 111, 110, 115, 101,  46]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Dict\n",
    "\n",
    "# Sample data\n",
    "genes = ['gene1', 'gene2', 'gene3']  # List of genes\n",
    "labels = [0, 1, 0]  # Corresponding labels for each gene\n",
    "descriptions = [\n",
    "    \"This gene is involved in metabolic processes.\",\n",
    "    \"This gene plays a role in cell division.\",\n",
    "    \"This gene is associated with the immune response.\"\n",
    "]  # Descriptions for each gene\n",
    "\n",
    "# Custom Dataset class\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, genes: List[str], labels: List[int], descriptions: List[str]):\n",
    "        self.genes = genes\n",
    "        self.labels = labels\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.genes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene = self.genes[idx]\n",
    "        label = self.labels[idx]\n",
    "        description = self.descriptions[idx]\n",
    "        return {\"gene\": gene, \"label\": torch.tensor(label), \"description\": description}\n",
    "\n",
    "# Custom collate function for variable-length descriptions\n",
    "def collate_fn(batch: List[Dict]):\n",
    "    genes = [item['gene'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    # Tokenize descriptions at character level\n",
    "    descriptions = [torch.tensor([ord(char) for char in item['description']], dtype=torch.long) for item in batch]\n",
    "    \n",
    "    # Pad descriptions to the same length\n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\"genes\": genes, \"labels\": labels, \"descriptions\": descriptions_padded}\n",
    "\n",
    "# Instantiate the dataset and data loader\n",
    "gene_dataset = GeneDataset(genes, labels, descriptions)\n",
    "gene_loader = DataLoader(gene_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Example usage\n",
    "for batch in gene_loader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
