{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "# Specify the working directory\n",
    "os.chdir('/Users/david/Desktop/FinetuneEmbed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a1c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gene ncbi descriptions\n",
    "with open(\"./data/gene_text/hs_ncbi_gene_text.json\", \"r\") as file:\n",
    "    gene_descriptions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e05ec",
   "metadata": {},
   "source": [
    "## Long- vs short- range TFs\n",
    "The input data used here are downloaded from Chen et al. (2020) (link: https://www-nature-com.stanford.idm.oclc.org/articles/s41467-020-16106-x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long_short_range_tf = pd.read_csv('./data/long_vs_shortTF/41467_2020_16106_MOESM4_ESM.csv')\n",
    "# long_range_tf_gene = list(long_short_range_tf[long_short_range_tf['assignment']=='long-range TF']\\\n",
    "#                                 ['Unnamed: 0'])\n",
    "# long_range_tf_gene = list(set(long_range_tf_gene) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "\n",
    "# short_range_tf_gene = list(long_short_range_tf[long_short_range_tf['assignment']=='short-range TF']\\\n",
    "#                                 ['Unnamed: 0'])\n",
    "# short_range_tf_gene = list(set(short_range_tf_gene) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59d202",
   "metadata": {},
   "source": [
    "The input data used here are downloaded from the geneformer paper Hugging Face website (link: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/example_input_files/gene_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/long_vs_shortTF/example_input_files_gene_classification_tf_regulatory_range_tf_regulatory_range.pickle\", \"rb\") as f:\n",
    "    check_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af92252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 input query terms found no hit:\t['ENSG00000269603', 'ENSG00000267841']\n"
     ]
    }
   ],
   "source": [
    "long_range_tf_gene = check_data['long_range']\n",
    "short_range_tf_gene = check_data['short_range']\n",
    "\n",
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "long_range_query = mg.querymany(long_range_tf_gene, species='human')\n",
    "short_range_query = mg.querymany(short_range_tf_gene, species='human')\n",
    "long_range_gene_name = [x['symbol'] for x in long_range_query]\n",
    "short_range_gene_name = [x['symbol'] for x in short_range_query if 'symbol' in x]\n",
    "\n",
    "long_range_tf_gene = list(set(long_range_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "short_range_tf_gene = list(set(short_range_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394d091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import GeneDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf_genes = long_range_tf_gene + short_range_tf_gene\n",
    "labels = [1] * len(long_range_tf_gene) + [0] * len(short_range_tf_gene) # 1 for long-range TF, 0 for short-range TF\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(tf_genes, labels, test_size=0.3, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ee70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'desc':desc_train, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'desc':desc_test, 'labels':labels_test}\n",
    "# Save as a pickle file\n",
    "with open(\"./data/long_vs_shortTF/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/long_vs_shortTF/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beed0917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9A4710B9-0DA3-36BB-9129-645F282E64B2> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <ECC148AF-20FF-3EEE-BC75-4DD3E7455393> /Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define your dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension added by the tokenizer (squeeze the single dimension)\n",
    "        encoding = {key: value.squeeze(0) for key, value in encoding.items()}\n",
    "        encoding[\"label\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return encoding\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Choose an appropriate Sentence BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Prepare datasets\n",
    "train_texts, train_labels = desc_train, labels_train   # your training texts and labels\n",
    "test_texts, test_labels = desc_test, labels_test    # your test texts and labels\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76e4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de97a68ba6314b939022d3cc8f201a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672e0b1969a1409aaa85908cdb4000f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5751219391822815, 'eval_AUC': 0.5021367521367521, 'eval_runtime': 0.203, 'eval_samples_per_second': 172.452, 'eval_steps_per_second': 24.636, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf5e471b738431ba8249f2d35d2ccc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6117882132530212, 'eval_AUC': 0.5, 'eval_runtime': 0.1876, 'eval_samples_per_second': 186.529, 'eval_steps_per_second': 26.647, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b6fbc31ee4632bf0bcbe722870712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.572949230670929, 'eval_AUC': 0.5598290598290598, 'eval_runtime': 0.1775, 'eval_samples_per_second': 197.193, 'eval_steps_per_second': 28.17, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d0c5f7aeac4177b05518ea1c2cf79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5788039565086365, 'eval_AUC': 0.5, 'eval_runtime': 0.1883, 'eval_samples_per_second': 185.875, 'eval_steps_per_second': 26.554, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7f3bf6745243e3964bd336886410d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5742241740226746, 'eval_AUC': 0.5, 'eval_runtime': 0.1787, 'eval_samples_per_second': 195.888, 'eval_steps_per_second': 27.984, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2893855bb7b54778ae994bb0d23609c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5736111402511597, 'eval_AUC': 0.4935897435897436, 'eval_runtime': 0.154, 'eval_samples_per_second': 227.283, 'eval_steps_per_second': 32.469, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a12a12388640e1890f4cd3876571dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5719671249389648, 'eval_AUC': 0.7649572649572649, 'eval_runtime': 0.1953, 'eval_samples_per_second': 179.192, 'eval_steps_per_second': 25.599, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154fe42d4d2741b3a89a1652dd9b605f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.576185941696167, 'eval_AUC': 0.8376068376068377, 'eval_runtime': 0.1818, 'eval_samples_per_second': 192.563, 'eval_steps_per_second': 27.509, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61641b8b737481191390a2e9bef9389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5721469521522522, 'eval_AUC': 0.5, 'eval_runtime': 0.1979, 'eval_samples_per_second': 176.861, 'eval_steps_per_second': 25.266, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2c63f9807d4de0bbcd9e164b5f1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5711102485656738, 'eval_AUC': 0.3589743589743589, 'eval_runtime': 0.174, 'eval_samples_per_second': 201.174, 'eval_steps_per_second': 28.739, 'epoch': 10.0}\n",
      "{'train_runtime': 28.1621, 'train_samples_per_second': 49.357, 'train_steps_per_second': 6.392, 'train_loss': 0.5899030897352431, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5775cdc4c5df4a2587157eeadec1851b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.3589743589743589\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the compute_metrics function for AUC\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()  # Get probability of the positive class\n",
    "    auc = roc_auc_score(labels, probs)\n",
    "    return {\"AUC\": auc}\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "results = trainer.evaluate()\n",
    "print(\"Test AUC:\", results[\"eval_AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f62f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab01726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b76c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88cbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b03ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac499cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556, 556, 556)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_train_aug), len(genes_train_aug), len(labels_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a394da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import collate_fn\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GeneDataset(genes_train_aug, desc_train_aug, labels_train_aug)\n",
    "val_dataset = GeneDataset(genes_test, desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train_aug, 'dataloader':train_loader, 'labels':labels_train_aug}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d2933a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m genes, labels, descs \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(genes)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/FinetuneEmbed/mod/mod.py:56\u001b[0m, in \u001b[0;36mGeneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     55\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescriptions[idx]\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m\"\u001b[39m: gene, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "for genes, labels, descs in train_loader:\n",
    "    print(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b700f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/long_vs_shortTF/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/long_vs_shortTF/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe13b8b",
   "metadata": {},
   "source": [
    "## Dosage sensitive vs insensitive TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_file = \"https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/raw/main/example_input_files/gene_classification/dosage_sensitive_tfs/dosage_sens_tf_labels.csv\"\n",
    "with open(f\"./data/DosageSensitivity/example_input_files_gene_classification_dosage_sensitive_tfs_dosage_sensitivity_TFs.pickle\", \"rb\") as fp:\n",
    "    dosage_tfs = pickle.load(fp)\n",
    "sensitive = dosage_tfs[\"Dosage-sensitive TFs\"]\n",
    "insensitive = dosage_tfs[\"Dosage-insensitive TFs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075909b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 input query terms found no hit:\t['ENSG00000215271']\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "sensitive_query = mg.querymany(sensitive, species='human')\n",
    "in_sensitive_query = mg.querymany(insensitive, species='human')\n",
    "sensitive_gene_name = [x['symbol'] for x in sensitive_query]\n",
    "insensitive_gene_name = [x['symbol'] for x in in_sensitive_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39656ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_gene = list(set(sensitive_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "insensitive_gene = list(set(insensitive_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = sensitive_gene + insensitive_gene\n",
    "labels = [1] * len(sensitive_gene) + [0] * len(insensitive_gene) # 1 for sensitive, 0 for insensitive\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/DosageSensitivity/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/DosageSensitivity/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083924a",
   "metadata": {},
   "source": [
    "## Methylation state prediction\n",
    "The csv files are downloaded from https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa2f48",
   "metadata": {},
   "source": [
    "#### Bivalent vs. lys4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/MethylationState/example_input_files_gene_classification_bivalent_promoters_bivalent_vs_lys4_only.pickle\", \"rb\") as fp:\n",
    "    bivalent_vs_lys4 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene_labels = bivalent_vs_lys4['bivalent']\n",
    "lysine_gene_labels = bivalent_vs_lys4['lys4_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c068c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 input query terms found dup hits:\t[('ENSG00000007372', 2), ('ENSG00000110693', 2), ('ENSG00000117707', 2), ('ENSG00000120093', 2), ('E\n",
      "2 input query terms found dup hits:\t[('ENSG00000196628', 2), ('ENSG00000198728', 2)]\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "bivalent_query = mg.querymany(bivalent_gene_labels, species='human')\n",
    "lysine_query = mg.querymany(lysine_gene_labels, species='human')\n",
    "bivalent_gene_name = [x.get('symbol', '') for x in bivalent_query]\n",
    "lysine_gene_name = [x.get('symbol', '') for x in lysine_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene = list(set(bivalent_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "lysine_gene = list(set(lysine_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c153053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = bivalent_gene + lysine_gene\n",
    "labels = [1] * len(bivalent_gene) + [0] * len(lysine_gene) # 1 for bivalent_gene, 0 for lysine_gene\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733796f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/MethylationState/bivalent_vs_lys4/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/MethylationState/bivalent_vs_lys4/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22624ec0",
   "metadata": {},
   "source": [
    "#### Bivalent vs. no methyl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62be879",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/MethylationState/example_input_files_gene_classification_bivalent_promoters_bivalent_vs_no_methyl.pickle\", \"rb\") as fp:\n",
    "    bivalent_vs_no_methyl = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1863d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene_labels = bivalent_vs_no_methyl['bivalent']\n",
    "no_methylation_gene_labels = bivalent_vs_no_methyl['no_methylation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 input query terms found dup hits:\t[('ENSG00000007372', 2), ('ENSG00000110693', 2), ('ENSG00000117707', 2), ('ENSG00000120093', 2), ('E\n",
      "2 input query terms found dup hits:\t[('ENSG00000147488', 2), ('ENSG00000151322', 2)]\n"
     ]
    }
   ],
   "source": [
    "import mygene\n",
    "# convert gene id to gene symbols\n",
    "mg = mygene.MyGeneInfo()\n",
    "bivalent_query = mg.querymany(bivalent_gene_labels, species='human')\n",
    "no_methylation_query = mg.querymany(no_methylation_gene_labels, species='human')\n",
    "bivalent_gene_name = [x.get('symbol', '') for x in bivalent_query]\n",
    "no_methylation_gene_name = [x.get('symbol', '') for x in no_methylation_query if 'symbol' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bivalent_gene = list(set(bivalent_gene_name) & set(gene_descriptions.keys())) # find the intersected genes\n",
    "no_methylation_gene = list(set(no_methylation_gene_name) & set(gene_descriptions.keys())) # find the intersected genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mod.mod import TextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "genes = bivalent_gene + no_methylation_gene\n",
    "labels = [1] * len(bivalent_gene) + [0] * len(no_methylation_gene) # 1 for bivalent_gene, 0 for no_methylation_gene\n",
    "# Split into train and test sets\n",
    "genes_train, genes_test, labels_train, labels_test = train_test_split(genes, labels, test_size=0.2, stratify=labels, random_state=7)\n",
    "\n",
    "desc_train = [gene_descriptions[gene] for gene in genes_train]\n",
    "desc_test = [gene_descriptions[gene] for gene in genes_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = TextDataset(desc_train, labels_train)\n",
    "val_dataset = TextDataset(desc_test, labels_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Save the data\n",
    "train_to_save = {'genes':genes_train, 'dataloader':train_loader, 'labels':labels_train}\n",
    "val_to_save = {'genes':genes_test, 'dataloader':val_loader, 'labels':labels_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a pickle file\n",
    "with open(\"./data/MethylationState/bivalent_vs_no_methyl/train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_to_save, f)\n",
    "with open(\"./data/MethylationState/bivalent_vs_no_methyl/test_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce388717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06603c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'genes': ['gene2', 'gene1'], 'labels': tensor([1, 0]), 'descriptions': tensor([[ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 112, 108,  97, 121,\n",
      "         115,  32,  97,  32, 114, 111, 108, 101,  32, 105, 110,  32,  99, 101,\n",
      "         108, 108,  32, 100, 105, 118, 105, 115, 105, 111, 110,  46,   0,   0,\n",
      "           0,   0,   0],\n",
      "        [ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 105, 115,  32, 105,\n",
      "         110, 118, 111, 108, 118, 101, 100,  32, 105, 110,  32, 109, 101, 116,\n",
      "          97,  98, 111, 108, 105,  99,  32, 112, 114, 111,  99, 101, 115, 115,\n",
      "         101, 115,  46]])}\n",
      "{'genes': ['gene3'], 'labels': tensor([0]), 'descriptions': tensor([[ 84, 104, 105, 115,  32, 103, 101, 110, 101,  32, 105, 115,  32,  97,\n",
      "         115, 115, 111,  99, 105,  97, 116, 101, 100,  32, 119, 105, 116, 104,\n",
      "          32, 116, 104, 101,  32, 105, 109, 109, 117, 110, 101,  32, 114, 101,\n",
      "         115, 112, 111, 110, 115, 101,  46]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List, Dict\n",
    "\n",
    "# Sample data\n",
    "genes = ['gene1', 'gene2', 'gene3']  # List of genes\n",
    "labels = [0, 1, 0]  # Corresponding labels for each gene\n",
    "descriptions = [\n",
    "    \"This gene is involved in metabolic processes.\",\n",
    "    \"This gene plays a role in cell division.\",\n",
    "    \"This gene is associated with the immune response.\"\n",
    "]  # Descriptions for each gene\n",
    "\n",
    "# Custom Dataset class\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, genes: List[str], labels: List[int], descriptions: List[str]):\n",
    "        self.genes = genes\n",
    "        self.labels = labels\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.genes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene = self.genes[idx]\n",
    "        label = self.labels[idx]\n",
    "        description = self.descriptions[idx]\n",
    "        return {\"gene\": gene, \"label\": torch.tensor(label), \"description\": description}\n",
    "\n",
    "# Custom collate function for variable-length descriptions\n",
    "def collate_fn(batch: List[Dict]):\n",
    "    genes = [item['gene'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    \n",
    "    # Tokenize descriptions at character level\n",
    "    descriptions = [torch.tensor([ord(char) for char in item['description']], dtype=torch.long) for item in batch]\n",
    "    \n",
    "    # Pad descriptions to the same length\n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\"genes\": genes, \"labels\": labels, \"descriptions\": descriptions_padded}\n",
    "\n",
    "# Instantiate the dataset and data loader\n",
    "gene_dataset = GeneDataset(genes, labels, descriptions)\n",
    "gene_loader = DataLoader(gene_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Example usage\n",
    "for batch in gene_loader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
